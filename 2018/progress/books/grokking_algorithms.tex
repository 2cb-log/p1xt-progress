\twocolumn
\chapter{Book: Grokking Algorithms}
\section{Notes}
\subsection{Introduction to Algorithms}
\subsubsection{Binary search}
\begin{itemize}
\item requires a sorted list of elements
\item returns the position within the list of the element being searched for
\item returns null if the element being searched for isn't present in the list
\item mechanic: eliminate half the list by comparing the search input to the element at the mid point of the list to determine whether the element is in the first half (is less than mid) or second half (is greater than mid). Continue eliminating half repeatedly until the search input equals the value at mid, or there are no more elements in the list to check (value does not exist in list).
\end{itemize}

\subsubsection{Logarithms}

\begin{itemize}
\item the flip side of exponents 
\item $\log_{2}8 == 3$ because $2^3 == 8$
\end{itemize}

\subsubsection{Big O notation}

\begin{itemize}
\item examines how fast an algorithm is 
\item compares the number of operations to determine how fast the algorithm grows 
\item establishes a worst case run time
\item efficiency, fastest to slowest
\begin{itemize} 
\item $O(log n)$ - log time (Binary search) 
\item $O(n)$ - linear time (Simple search) 
\item $O(n * log n)$ - (quicksort) 
\item $O(n^2)$ - (selection sort) 
\item $O(n!)$ - (travelling salesman) 
\end{itemize}
\end{itemize}
\subsection{Selection sort}
\begin{itemize}
\item $O(n^2)$
\item find biggest item in list, add to new list
\item repeat until all items transferred to new list
\end{itemize}
\subsection{Arrays and linked lists}

\begin{itemize}
\item Arrays
\begin{itemize}
\item Allow fast reads
\end{itemize}
\item Linked lists
\item All elements must be the same type
\item All elements are stored contiguously \\ in memory
\begin{itemize}
\item Allow fast inserts
\item Allow fast deletes
\item Each item is stored independently in memory
\end{itemize}
\end{itemize}


\subsection{Recursion}
A recursive function calls itself, passing the new instance a smaller subset of the same problem. The function must be supplied a "base case" to indicate that some value be returned rather than a new recursion so as to avoid an infinite loop.
\subsection{Quicksort}

\begin{itemize}
\item Divide and Conquer
\item Base case - simplest case
\item Divide problem until it becomes the base case
\item Quicksort - pick a pivot, create two \\ sub-arrays, elements smaller and larger than pivot, \\ quicksort recursively on the sub-arrays.
\end{itemize}

\textbf{Inductive proof} has base case and inductive case. (if it works for base case, it will work for one larger than base, if it works for that, it will work for all.

\subsection{Hash tables} Maps strings to numbers. Is a dictionary in \\ Python. Is good for lookups. Useful for caching. Needs a good hash function to ensure keys map evenly over the hash. Take constant O(1) time. Avoid collisions with a low load factor (items/slots) and a good hash function.

\subsection{Breadth-first search} Allows you to find the shortest path. A graph models a set of connections as a set of nodes with edges that can be connected to other nodes (neighbors). Good for answering two types of questions: 

\begin{itemize}
\item Is there a path between two items.
\item What is the shortest path between two items.
\end{itemize}

Queue is FIFO. Stack is LIFO.
A tree is a unidirectional graph.

\subsection{Dijkstra's algorithm} Finds fastest path (as opposed to the shortest path from breadth-first). Find cheapest node, check if there is cheaper path to neighbors (if so update their costs), repeat for every node, calculate final path. Only works on directed acyclic graphs (DAGs)

\begin{lstlisting}[language=Python]
node = find_lowest_cost_node(costs)
while node is not None:
	cost = costs[node]
	neighbors = graph[node]
	for n in neighbors.keys():
		new_cost = cost + neighbors[n]
		if costs[n] > new_cost:
			costs[n] = new_cost
			parents[n[ = node
	processed.append(node)
	node = find_lowest_cost_node(costs)
		
\end{lstlisting}

\subsection{Greedy algorithms} Used to tackle problems that don't have a fast algorithmic solution (np-complete problems). Simple to write and get "pretty close". Optimize locally, hoping to end up with an overall optimal solution. If you have an NP-complete problem, your best bet is to use an approximation algorithm. Greedy algorithms run fast and are easy to write so they are good options for approximation algorithms.
\subsection{Dynamic programming} Solve sub-problem which leads to solution to entire problem. Doesn't operate on fractions, either takes the whole or nothing. Each subproblem must be discrete. Sometimes best solution doesn't fill knapsack completely. Use a grid to solve, each cell is a sub-problem.

\subsection{K-nearest neighbors} used for classification (categorization in a group) and regression (predicting a response). Feature extraction means converting an item into a list of numbers that can be compared. Pick good features for success.
\subsection{Where to go next} The Fourier transform, given a thing, it will give you the ingredients (sub parts) of that thing. Distributed algorithms, can run across multiple machines.

Note: I WAS doing all of the problems. But they were trivial. So I stopped. This was a decent overview book but it pretty much stayed in beginner-land, I'll do exercises when I hit up Cormen or Sedgewick.

